{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to sofes","text":"<p>A python package for prototype-based soft feature selection</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://naotoo1.github.io/sofes</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/naotoo1/sofes/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>sofes could always use more documentation, whether as part of the official sofes docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/naotoo1/sofes/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up sofes for local development.</p> <ol> <li> <p>Fork the sofes repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/sofes.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv sofes\n$ cd sofes/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 sofes tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/naotoo1/sofes/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install sofes, run this command in your terminal:</p> <pre><code>pip install sofes\n</code></pre> <p>This is the preferred method to install sofes, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install sofes from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/naotoo1/sofes\n</code></pre>"},{"location":"sofes/","title":"sofes module","text":"<p>Implementation of Soft Prototype Feature Selection  Algorithm</p>"},{"location":"sofes/#sofes.sofes.BestLearnedResults","title":"<code> BestLearnedResults        </code>  <code>dataclass</code>","text":"<p>BestLearnedResults(omega_matrix: List[numpy.ndarray], evaluation_metric_score: List[float], num_prototypes: int)</p> Source code in <code>sofes/sofes.py</code> <pre><code>class BestLearnedResults:\n    omega_matrix: List[np.ndarray]\n    evaluation_metric_score: List[float]\n    num_prototypes: int\n</code></pre>"},{"location":"sofes/#sofes.sofes.GlobalFeatureSelection","title":"<code> GlobalFeatureSelection        </code>  <code>dataclass</code>","text":"<p>GlobalFeatureSelection(relevance: sofes.sofes.GlobalRelevanceFactorsSummary, eval_score: List[float], num_prototypes: int)</p> Source code in <code>sofes/sofes.py</code> <pre><code>class GlobalFeatureSelection:\n    relevance: GlobalRelevanceFactorsSummary\n    eval_score: List[float]\n    num_prototypes: int\n</code></pre>"},{"location":"sofes/#sofes.sofes.GlobalRelevanceFactorsSummary","title":"<code> GlobalRelevanceFactorsSummary        </code>  <code>dataclass</code>","text":"<p>GlobalRelevanceFactorsSummary(omega_matrix: numpy.ndarray, lambda_matrix: numpy.ndarray, lambda_diagonal: numpy.ndarray, lambda_row_sum: numpy.ndarray, feature_relevance_dict: Dict[str, Any], weight_significance: numpy.ndarray)</p> Source code in <code>sofes/sofes.py</code> <pre><code>class GlobalRelevanceFactorsSummary:\n    omega_matrix: np.ndarray\n    lambda_matrix: np.ndarray\n    lambda_diagonal: np.ndarray\n    lambda_row_sum: np.ndarray\n    feature_relevance_dict: Dict[str, Any]\n    weight_significance: np.ndarray\n</code></pre>"},{"location":"sofes/#sofes.sofes.HitsInfo","title":"<code> HitsInfo        </code>  <code>dataclass</code>","text":"<p>HitsInfo(features: List[int], hits: List[int])</p> Source code in <code>sofes/sofes.py</code> <pre><code>class HitsInfo:\n    features: List[int]\n    hits: List[int]\n</code></pre>"},{"location":"sofes/#sofes.sofes.LVQ","title":"<code> LVQ            (str, Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>sofes/sofes.py</code> <pre><code>class LVQ(str, Enum):\n    MRSLVQ = \"mrslvq\"\n    LMRSLVQ = \"lmrslvq\"\n</code></pre>"},{"location":"sofes/#sofes.sofes.LocalFeatureSelection","title":"<code> LocalFeatureSelection        </code>  <code>dataclass</code>","text":"<p>LocalFeatureSelection(relevance: sofes.sofes.LocalRelevanceFactorSummary, eval_score: List[float], num_prototypes: int)</p> Source code in <code>sofes/sofes.py</code> <pre><code>class LocalFeatureSelection:\n    relevance: LocalRelevanceFactorSummary\n    eval_score: List[float]\n    num_prototypes: int\n</code></pre>"},{"location":"sofes/#sofes.sofes.LocalRejectStrategy","title":"<code> LocalRejectStrategy        </code>  <code>dataclass</code>","text":"<p>LocalRejectStrategy(significant: List[int], insignificant: List[int], significant_hit: List[int], insignificant_hit: List[int], tentative: Union[List[int], NoneType])</p> Source code in <code>sofes/sofes.py</code> <pre><code>class LocalRejectStrategy:\n    significant: List[int]\n    insignificant: List[int]\n    significant_hit: List[int]\n    insignificant_hit: List[int]\n    tentative: Union[List[int], None]\n</code></pre>"},{"location":"sofes/#sofes.sofes.LocalRelevanceFactorSummary","title":"<code> LocalRelevanceFactorSummary        </code>  <code>dataclass</code>","text":"<p>LocalRelevanceFactorSummary(omega_matrix: List[numpy.ndarray], lambda_matrix: List[numpy.ndarray], lambda_diagonal: List[numpy.ndarray], lambda_row_sum: numpy.ndarray, feature_relevance_dict: Dict[str, Any], weight_significance: numpy.ndarray, feature_significance: numpy.ndarray)</p> Source code in <code>sofes/sofes.py</code> <pre><code>class LocalRelevanceFactorSummary:\n    omega_matrix: List[np.ndarray]\n    lambda_matrix: List[np.ndarray]\n    lambda_diagonal: List[np.ndarray]\n    lambda_row_sum: np.ndarray\n    feature_relevance_dict: Dict[str, Any]\n    weight_significance: np.ndarray\n    feature_significance: np.ndarray\n</code></pre>"},{"location":"sofes/#sofes.sofes.SavedModelUpdate","title":"<code> SavedModelUpdate            (str, Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>sofes/sofes.py</code> <pre><code>class SavedModelUpdate(str, Enum):\n    TRUE = \"update\"\n    FALSE = \"keine-update\"\n</code></pre>"},{"location":"sofes/#sofes.sofes.SelectedRelevances","title":"<code> SelectedRelevances        </code>  <code>dataclass</code>","text":"<p>SelectedRelevances(significant: Union[List[int], List[str], numpy.ndarray], insignificant: Union[List[int], List[str], numpy.ndarray])</p> Source code in <code>sofes/sofes.py</code> <pre><code>class SelectedRelevances:\n    significant: Union[List[int], List[str], np.ndarray]\n    insignificant: Union[List[int], List[str], np.ndarray]\n</code></pre>"},{"location":"sofes/#sofes.sofes.SelectedRelevancesExtra","title":"<code> SelectedRelevancesExtra        </code>  <code>dataclass</code>","text":"<p>SelectedRelevancesExtra(significant: sofes.sofes.HitsInfo, insignificant: sofes.sofes.HitsInfo)</p> Source code in <code>sofes/sofes.py</code> <pre><code>class SelectedRelevancesExtra:\n    significant: HitsInfo\n    insignificant: HitsInfo\n</code></pre>"},{"location":"sofes/#sofes.sofes.SofesPy","title":"<code> SofesPy        </code>  <code>dataclass</code>","text":"<p>SofesPy(input_data: numpy.ndarray, labels: numpy.ndarray, model_name: str, latent_dim: int, sigma: int, num_classes: int, init_prototypes: Union[numpy.ndarray, NoneType], init_matrix: Union[numpy.ndarray, NoneType], num_prototypes: int, random_state: Union[int, NoneType], feature_list: Union[List[str], NoneType] = None, eval_type: Union[str, NoneType] = None, regularization: float = 0, max_epochs: int = 10, save_model: bool = False, significance: bool = True, perturbation_distribution: str = 'balanced', perturbation_ratio: float = 0.2, evaluation_metric: str = 'accuracy', epsilon: Union[float, NoneType] = 0.0001, norm_ord: str = 'fro', termination: str = 'metric', patience: int = 1, verbose: int = 0, summary_metric_list: list = ) Source code in <code>sofes/sofes.py</code> <pre><code>class SofesPy:\n    input_data: np.ndarray\n    labels: np.ndarray\n    model_name: str\n    latent_dim: int\n    sigma:int\n    num_classes: int\n    init_prototypes:Union[np.ndarray,None]\n    init_matrix:Union[np.ndarray,None]\n    num_prototypes: int\n    random_state:Union[int,None]\n    feature_list: Union[List[str], None] = None\n    eval_type: Union[str, None] = None\n    regularization: float = 0\n    max_epochs: int = 10\n    save_model: bool = False\n    significance: bool = True\n    perturbation_distribution: str = \"balanced\"\n    perturbation_ratio: float = 0.2\n    evaluation_metric: str = EvaluationMetricsType.ACCURACY.value\n    epsilon: Union[float, None] = 0.0001\n    norm_ord: str = \"fro\"\n    termination: str = \"metric\"\n    patience: int = 1\n    verbose: int = 0\n    summary_metric_list: list = field(default_factory=lambda: [])\n\n    def train_ho(self, increment: int) -&gt; TrainModelSummary:\n        return train_hold_out(\n            input_data=self.input_data,\n            labels=self.labels,\n            model_name=self.model_name,\n            latent_dim=self.latent_dim,\n            sigma=self.sigma,\n            init_prototypes=self.init_prototypes,\n            num_prototypes=self.num_prototypes + increment,\n            # save_model=self.save_model,\n            evaluation_metric=self.evaluation_metric,\n            max_iter=self.max_epochs,\n            random_state=self.random_state\n        )\n\n    def train_mv(self, increment: int) -&gt; TrainModelSummary:\n        return train_model_by_mv(\n            input_data=self.input_data,\n            labels=self.labels,\n            sigma=self.sigma,\n            model_name=self.model_name,\n            latent_dim=self.latent_dim,\n            num_prototypes=self.num_prototypes + increment,\n            # save_model=self.save_model,\n            evaluation_metric=self.evaluation_metric,\n            perturbation_distribution=self.perturbation_distribution,\n            perturbation_ratio=self.perturbation_ratio,\n            max_iter=self.max_epochs,\n            random_state=self.random_state\n        )\n\n    @property\n    def final(self) -&gt; BestLearnedResults:  # type: ignore\n        (metric_list, matrix_list, should_continue, counter) = ([], [], True, -1)\n        while should_continue:\n            counter += 1\n            train_eval_scheme = (\n                self.train_mv(increment=counter)\n                if self.eval_type == ValidationType.MUTATEDVALIDATION.value\n                else self.train_ho(increment=counter)\n            )\n            validation_score = (\n                train_eval_scheme.selected_model_evaluation_metrics_scores\n            )\n            omega_matrix = train_eval_scheme.final_omega_matrix\n            num_prototypes = (\n                    len((train_eval_scheme.final_prototypes[0])) // self.num_classes\n            )\n            metric_list.append(validation_score[0])\n            matrix_list.append(omega_matrix[0])\n            if counter &lt; self.patience:\n                continue\n            condition = counter == self.max_epochs\n            stability = get_stability(\n                metric1=metric_list[-2],\n                metric2=metric_list[-1],\n                matrix1=matrix_list[-2],\n                matrix2=matrix_list[-1],\n                convergence=self.termination,\n                epsilon=self.epsilon,\n                learner=self.model_name,\n                matrix_ord=self.norm_ord,\n            )\n            if (\n                    condition is False\n                    and self.termination == \"metric\"\n                    and stability is True\n            ):\n                should_continue = False\n                return BestLearnedResults(\n                    omega_matrix=omega_matrix,\n                    evaluation_metric_score=validation_score,\n                    num_prototypes=num_prototypes,\n                )\n            if (\n                    condition is False\n                    and self.termination == \"matrix\"\n                    and stability is True\n            ):\n                should_continue = False\n                return BestLearnedResults(\n                    omega_matrix=omega_matrix,\n                    evaluation_metric_score=validation_score,\n                    num_prototypes=num_prototypes,\n                )\n            if (\n                    condition is True\n                    and self.termination == \"metric\"\n                    and stability is True\n            ):\n                should_continue = False\n                return BestLearnedResults(\n                    omega_matrix=omega_matrix,\n                    evaluation_metric_score=validation_score,\n                    num_prototypes=num_prototypes,\n                )\n            if (\n                    condition is True\n                    and self.termination == \"matrix\"\n                    and stability is True\n            ):\n                should_continue = False\n                return BestLearnedResults(\n                    omega_matrix=omega_matrix,\n                    evaluation_metric_score=validation_score,\n                    num_prototypes=num_prototypes,\n                )\n\n    @property\n    def feature_selection(self) -&gt; Union[GlobalFeatureSelection, LocalFeatureSelection]:\n        if (\n                self.model_name == LVQ.MRSLVQ\n                and self.eval_type == ValidationType.MUTATEDVALIDATION.value\n        ):\n            train_eval_scheme = self.final\n            validation_score = train_eval_scheme.evaluation_metric_score\n            omega_matrix = train_eval_scheme.omega_matrix\n            relevance = get_lambda_matrix(\n                omega_matrix=omega_matrix, feature_list=self.feature_list\n            )\n            return GlobalFeatureSelection(\n                relevance=relevance,\n                eval_score=validation_score,\n                num_prototypes=train_eval_scheme.num_prototypes,\n            )\n        if (\n                self.model_name == LVQ.MRSLVQ\n                and self.eval_type == ValidationType.HOLDOUT.value\n        ):\n            train_eval_scheme = self.final\n            validation_score = train_eval_scheme.evaluation_metric_score\n            omega_matrix = train_eval_scheme.omega_matrix\n            relevance = get_lambda_matrix(\n                omega_matrix=omega_matrix, feature_list=self.feature_list\n            )\n            return GlobalFeatureSelection(\n                relevance=relevance,\n                eval_score=validation_score,\n                num_prototypes=train_eval_scheme.num_prototypes,\n            )\n\n        if (\n                self.model_name == LVQ.LMRSLVQ\n                and self.eval_type == ValidationType.MUTATEDVALIDATION.value\n        ):\n            train_eval_scheme = self.final\n            validation_score = train_eval_scheme.evaluation_metric_score\n            omega_matrix = train_eval_scheme.omega_matrix\n            num_prototypes = train_eval_scheme.num_prototypes\n            relevance = get_local_lambda_matrix(\n                omega_matrix=omega_matrix,\n                feature_list=self.feature_list,\n                num_prototypes=num_prototypes,\n                num_classes=self.num_classes,\n            )\n            return LocalFeatureSelection(\n                relevance=relevance,\n                eval_score=validation_score,\n                num_prototypes=num_prototypes,\n            )\n\n        if (\n                self.model_name == LVQ.LMRSLVQ\n                and self.eval_type == ValidationType.HOLDOUT.value\n        ):\n            train_eval_scheme = self.final\n            validation_score = train_eval_scheme.evaluation_metric_score\n            omega_matrix = train_eval_scheme.omega_matrix\n            num_prototypes = train_eval_scheme.num_prototypes\n            relevance = get_local_lambda_matrix(\n                omega_matrix=omega_matrix,\n                feature_list=self.feature_list,\n                num_classes=self.num_classes,\n                num_prototypes=num_prototypes,\n            )\n            return LocalFeatureSelection(\n                relevance=relevance,\n                eval_score=validation_score,\n                num_prototypes=num_prototypes,\n            )\n        raise RuntimeError(\n            \"feature_selection: none of the above cases match\"\n            )\n\n    @property\n    def summary_results(self):\n        feature_selection = self.feature_selection\n        if self.model_name == LVQ.LMRSLVQ and self.significance is True:\n            summary = get_relevance_summary(\n                feature_significance=feature_selection.relevance.feature_significance,  \n                evaluation_metric_score=feature_selection.eval_score[0],\n                verbose=self.verbose,\n            )\n            return SelectedRelevances(\n                significant=summary.significant,\n                insignificant=summary.insignificant,\n            )\n        if self.model_name == LVQ.LMRSLVQ and self.significance is False:\n            summary = get_relevance_elimination_summary(\n                weight_significance=feature_selection.relevance.lambda_row_sum,\n                num_protypes_per_class=self.feature_selection.num_prototypes,\n                lambda_row_sum=feature_selection.relevance.lambda_row_sum,\n                evaluation_metric_score=feature_selection.eval_score[0],\n                verbose=self.verbose,\n                input_dim=self.latent_dim,\n                num_classes=self.num_classes,\n            )\n\n            visualize(\n                features=summary.significant.features,\n                hits=summary.significant.hits,\n                significance=True,\n                eval_score=feature_selection.eval_score[0],\n            )\n            visualize(\n                features=summary.insignificant.features,\n                hits=summary.insignificant.hits,\n                significance=False,\n                eval_score=feature_selection.eval_score[0],\n            )\n            return SelectedRelevancesExtra(\n                significant=summary.significant,\n                insignificant=summary.insignificant,\n            )\n        if self.model_name == LVQ.MRSLVQ and self.significance is True:\n            summary = get_relevance_global_summary(\n                lambda_row_sum=feature_selection.relevance.lambda_row_sum,\n                weight_significance=feature_selection.relevance.weight_significance,\n                significance=self.significance,\n                evaluation_metric_score=feature_selection.eval_score[0],\n                verbose=self.verbose,\n            )\n            return SelectedRelevances(\n                significant=np.array(summary.significant).flatten(),\n                insignificant=np.array(summary.insignificant).flatten(),\n            )\n        if self.model_name == LVQ.MRSLVQ and self.significance is False:\n            summary = get_relevance_global_summary(\n                lambda_row_sum=feature_selection.relevance.lambda_row_sum,\n                weight_significance=feature_selection.relevance.weight_significance,\n                significance=self.significance,\n                evaluation_metric_score=feature_selection.eval_score[0],\n                verbose=self.verbose,\n            )\n            return SelectedRelevances(\n                significant=np.array(summary.significant).flatten(),\n                insignificant=np.array(summary.insignificant).flatten(),\n            )\n        raise RuntimeError(\"summary_results: none of the above cases match\")\n</code></pre>"},{"location":"sofes/#sofes.sofes.TrainModelSummary","title":"<code> TrainModelSummary        </code>  <code>dataclass</code>","text":"<p>TrainModelSummary(selected_model_evaluation_metrics_scores: List[float], final_omega_matrix: List[numpy.ndarray], final_prototypes: List[numpy.ndarray])</p> Source code in <code>sofes/sofes.py</code> <pre><code>class TrainModelSummary:\n    selected_model_evaluation_metrics_scores: List[float]\n    final_omega_matrix: List[np.ndarray]\n    final_prototypes: List[np.ndarray]\n</code></pre>"},{"location":"sofes/#sofes.sofes.ValidationType","title":"<code> ValidationType            (str, Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>sofes/sofes.py</code> <pre><code>class ValidationType(str, Enum):\n    MUTATEDVALIDATION = \"mv\"\n    HOLDOUT = \"ho\"\n</code></pre>"},{"location":"sofes/#sofes.sofes.Verbose","title":"<code> Verbose            (int, Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>sofes/sofes.py</code> <pre><code>class Verbose(int, Enum):\n    YES = 0\n    NO = 1\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use sofes in a project:</p> <pre><code>import sofes\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"}]}